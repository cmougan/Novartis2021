{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:09.105911Z",
     "start_time": "2021-11-26T18:44:09.094730Z"
    }
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:09.153073Z",
     "start_time": "2021-11-26T18:44:09.110662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension(\"collapsible_headings/main\")\n",
       "utils.load_extension(\"hide_input/main\")\n",
       "utils.load_extension(\"autosavetime/main\")\n",
       "utils.load_extension(\"execute_time/ExecuteTime\")\n",
       "utils.load_extension(\"code_prettify/code_prettify\")\n",
       "utils.load_extension(\"scroll_down/main\")\n",
       "utils.load_extension(\"jupyter-js-widgets/extension\")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension(\"collapsible_headings/main\")\n",
    "utils.load_extension(\"hide_input/main\")\n",
    "utils.load_extension(\"autosavetime/main\")\n",
    "utils.load_extension(\"execute_time/ExecuteTime\")\n",
    "utils.load_extension(\"code_prettify/code_prettify\")\n",
    "utils.load_extension(\"scroll_down/main\")\n",
    "utils.load_extension(\"jupyter-js-widgets/extension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:09.169622Z",
     "start_time": "2021-11-26T18:44:09.162672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cmougan/Desktop/Novartis2021/NN_files\n"
     ]
    }
   ],
   "source": [
    "cd NN_files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:09.825050Z",
     "start_time": "2021-11-26T18:44:09.175225Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from metrics.metric_participants import ComputeMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:13.782882Z",
     "start_time": "2021-11-26T18:44:09.827646Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from category_encoders import TargetEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from nnet import ReadDataset, Net,ResNet\n",
    "import time\n",
    "from loss_functions import interval_score_loss\n",
    "\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from gauss_rank_scaler import GaussRankScaler\n",
    "tic = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:13.811461Z",
     "start_time": "2021-11-26T18:44:13.798253Z"
    }
   },
   "outputs": [],
   "source": [
    "def curation_post(data):\n",
    "    aux = data.copy()\n",
    "\n",
    "    # Save arrays\n",
    "    aux_low = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_low\n",
    "    aux_high = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_high\n",
    "    aux_index = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].index\n",
    "\n",
    "    # Modify\n",
    "    aux.loc[aux_index, \"pred_95_low\"] = aux_high\n",
    "    aux.loc[aux_index, \"pred_95_high\"] = aux_low\n",
    "    \n",
    "    \n",
    "    if aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape[0]>0:\n",
    "        print('If errors they should appear: ')\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "\n",
    "    preds_aux = np.mean([aux.pred_95_low, aux.pred_95_high], axis=0)\n",
    "    \n",
    "    aux['prediction'] = preds_aux\n",
    "    return aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:13.868358Z",
     "start_time": "2021-11-26T18:44:13.858751Z"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess_submission(submission_df, solve_submission_issues=True):\n",
    "\n",
    "    join_on = [\"country\", \"brand\", \"month_num\"]\n",
    "    keep = join_on + [\"volume\"]\n",
    "\n",
    "    df_vol = pd.read_csv(\"../data/gx_volume.csv\").loc[:, keep]\n",
    "\n",
    "    both_ds = submission_df.merge(\n",
    "        df_vol,\n",
    "        on=join_on,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    both_ds.loc[both_ds[\"volume\"].notnull(), \"prediction\"] = both_ds[both_ds[\"volume\"].notnull()][\"volume\"].values\n",
    "    both_ds.loc[both_ds[\"volume\"].notnull(), \"pred_95_high\"] = both_ds[both_ds[\"volume\"].notnull()][\"volume\"].values + 0.01\n",
    "    both_ds.loc[both_ds[\"volume\"].notnull(), \"pred_95_low\"] = both_ds[both_ds[\"volume\"].notnull()][\"volume\"].values - 0.01\n",
    "\n",
    "    final_cols = join_on + [\"pred_95_low\", \"prediction\", \"pred_95_high\"]\n",
    "\n",
    "    final_df =  both_ds.loc[:, final_cols]\n",
    "\n",
    "    if solve_submission_issues:\n",
    "\n",
    "        if (final_df.pred_95_low > final_df.pred_95_high).any():\n",
    "            raise(\"Stop please, upper < lower\")\n",
    "\n",
    "        cond_lower_mean = final_df.pred_95_low > final_df.prediction\n",
    "        if cond_lower_mean.any():\n",
    "            print(\"Solving lower > mean\")\n",
    "            final_df.loc[cond_lower_mean, \"prediction\"] = \\\n",
    "                final_df.loc[cond_lower_mean, \"pred_95_low\"] + 0.01\n",
    "\n",
    "        cond_upper_mean = final_df.prediction > final_df.pred_95_high\n",
    "        if cond_upper_mean.any():\n",
    "            print(\"Solving upper < mean\")\n",
    "            final_df.loc[cond_upper_mean, \"prediction\"] = \\\n",
    "                final_df.loc[cond_upper_mean, \"pred_95_high\"] - 0.01\n",
    "\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:13.911935Z",
     "start_time": "2021-11-26T18:44:13.875265Z"
    }
   },
   "outputs": [],
   "source": [
    "def my_metric(pred, lower, upper):\n",
    "\n",
    "    metric_pair = compute_metrics(\n",
    "        preds=pred,\n",
    "        lower=lower,\n",
    "        upper=upper,\n",
    "        y=val_y_raw,\n",
    "        offset=val_offset,\n",
    "        X=val_x_orig,\n",
    "        avg_volumes=avg_volumes,\n",
    "    )\n",
    "    return metric_pair[0],metric_pair[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:14.497046Z",
     "start_time": "2021-11-26T18:44:13.918207Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from eda.checker import check_train_test\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "sales_train = pd.read_csv(\"../data/data_raw/sales_train.csv\")\n",
    "df_full = pd.read_csv(\"../data/split.csv\")\n",
    "df_region = pd.read_csv(\"../data/data_raw/regions.csv\")\n",
    "regions_hcps = pd.read_csv(\"../data/data_raw/regions_hcps.csv\")\n",
    "activity_features = pd.read_csv(\"../data/features/activity_features.csv\")\n",
    "brands_3_12 = pd.read_csv(\"../data/features/brand_3_12_market_features_lagged.csv\")\n",
    "rte_basic = pd.read_csv(\"../data/features/rte_basic_features.csv\").drop(\n",
    "    columns=[\"sales\", \"validation\"]\n",
    ")\n",
    "\n",
    "market_size = pd.read_csv(\"../data/market_size.csv\")\n",
    "\n",
    "# For reproducibility\n",
    "random.seed(0)\n",
    "VAL_SIZE = 38\n",
    "SUBMISSION_NAME = \"empty_extractor_target_encoder\"\n",
    "RETRAIN = True\n",
    "\n",
    "# %% Training weights\n",
    "market_size = (\n",
    "    market_size\n",
    "    .assign(weight=lambda x: 100 / x['sales'])\n",
    "    .rename(columns={\"sales\": 'market_size'})\n",
    ")\n",
    "\n",
    "market_size\n",
    "\n",
    "# %% Add region data\n",
    "df_feats = df_full.merge(df_region, on=\"region\", how=\"left\")\n",
    "df_feats = pd.merge(left=df_feats, right=regions_hcps, how=\"left\", on=\"region\")\n",
    "df_feats = df_feats.merge(\n",
    "    activity_features, on=[\"month\", \"region\", \"brand\"], how=\"left\"\n",
    ")\n",
    "df_feats = df_feats.merge(rte_basic, on=[\"month\", \"region\", \"brand\"], how=\"left\")\n",
    "df_feats = df_feats.merge(brands_3_12, on=[\"month\", \"region\"], how=\"left\")\n",
    "df_feats[\"whichBrand\"] = np.where(df_feats.brand == \"brand_1\", 1, 0)\n",
    "\n",
    "df_feats = df_feats.merge(market_size, on='region', how=\"left\")\n",
    "\n",
    "df_feats['month_brand'] = df_feats.month + '_' + df_feats.brand\n",
    "\n",
    "# drop sum variables\n",
    "cols_to_drop = [\"region\", \"sales\", \"validation\", \"market_size\", \"weight\"]\n",
    "\n",
    "# %% Split train val test\n",
    "X_train = df_feats.query(\"validation == 0\").drop(columns=cols_to_drop)\n",
    "y_train = df_feats.query(\"validation == 0\").sales\n",
    "weights_train = df_feats.query(\"validation == 0\").weight\n",
    "\n",
    "X_val = df_feats.query(\"validation == 1\").drop(columns=cols_to_drop)\n",
    "y_val = df_feats.query(\"validation == 1\").sales\n",
    "\n",
    "X_full = df_feats.query(\"validation.notnull()\", engine=\"python\").drop(\n",
    "    columns=cols_to_drop\n",
    ")\n",
    "y_full = df_feats.query(\"validation.notnull()\", engine=\"python\").sales\n",
    "weights_full = df_feats.query(\"validation.notnull()\", engine=\"python\").weight\n",
    "\n",
    "X_test = df_feats.query(\"validation.isnull()\", engine=\"python\").drop(\n",
    "    columns=cols_to_drop\n",
    ")\n",
    "y_test = df_feats.query(\"validation.isnull()\", engine=\"python\").sales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.168199Z",
     "start_time": "2021-11-26T18:44:14.499108Z"
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "from sklego.preprocessing import ColumnSelector\n",
    "from sktools import IsEmptyExtractor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.175513Z",
     "start_time": "2021-11-26T18:44:15.170771Z"
    }
   },
   "outputs": [],
   "source": [
    "select_cols = [\n",
    "    'whichBrand',\n",
    "    # 'Internal medicine',\n",
    "    # 'Pediatrician',\n",
    "    # 'null_tiers_Internal medicine',\n",
    "    'count',\n",
    "    'inverse_tier_f2f',\n",
    "    'hcp_distinct_Internal medicine / pneumology',\n",
    "    'sales_brand_3',\n",
    "    'sales_brand_3_market',\n",
    "    'sales_brand_12_market',\n",
    "    'month_brand',\n",
    "    'month',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.182072Z",
     "start_time": "2021-11-26T18:44:15.177903Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "pipe = Pipeline(\n",
    "        [   \n",
    "            (\"te\", TargetEncoder(cols=[\"month_brand\", \"month\", \"brand\"])),\n",
    "            (\"selector\", ColumnSelector(columns=select_cols)),\n",
    "            (\"empty\", IsEmptyExtractor()),\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "            (\"lgb\", model)\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.250939Z",
     "start_time": "2021-11-26T18:44:15.184448Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.291359Z",
     "start_time": "2021-11-26T18:44:15.253096Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pipe[:-1].transform(X_train)\n",
    "X_val = pipe[:-1].transform(X_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.303887Z",
     "start_time": "2021-11-26T18:44:15.293084Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussRankScaler()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = GaussRankScaler()\n",
    "scaler.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.320793Z",
     "start_time": "2021-11-26T18:44:15.308365Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:05:04.647134Z",
     "start_time": "2021-11-26T18:05:04.627931Z"
    }
   },
   "source": [
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train.values)\n",
    "X_val = torch.tensor(X_val)\n",
    "y_val = torch.tensor(y_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.328814Z",
     "start_time": "2021-11-26T18:44:15.322622Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReadDataset(Dataset):\n",
    "    \"\"\"Read dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, XX,yy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Path to the csv file with the students data.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = XX\n",
    "        self.y = yy\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __shape__(self):\n",
    "        return self.X.shape[1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert idx from tensor to list due to pandas bug (that arises when using pytorch's random_split)\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        self.X.iloc[idx].values\n",
    "        self.y[idx]\n",
    "\n",
    "        return [self.X.iloc[idx].values, self.y[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.334566Z",
     "start_time": "2021-11-26T18:44:15.330694Z"
    }
   },
   "outputs": [],
   "source": [
    "trainset = ReadDataset(pd.DataFrame(X_train),y_train.values)\n",
    "testset = ReadDataset(pd.DataFrame(X_val),y_val.values)\n",
    "\n",
    "\n",
    "# Data loaders\n",
    "trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "# Test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.346780Z",
     "start_time": "2021-11-26T18:44:15.336700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = torch.tensor(trainset.X.values)\n",
    "y_train = torch.tensor(trainset.y)\n",
    "\n",
    "\n",
    "\n",
    "X_test = torch.tensor(testset.X.values)\n",
    "y_test = torch.tensor(testset.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.352579Z",
     "start_time": "2021-11-26T18:44:15.348413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use gpu if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:15.365864Z",
     "start_time": "2021-11-26T18:44:15.354588Z"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "nnet = ResNet(trainset.__shape__()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:50:27.349605Z",
     "start_time": "2021-11-26T18:50:27.345084Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    nnet.parameters(), lr=0.00001, betas=(0.9, 0.999), eps=1e-08,  # weight_decay=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the net\n",
    "loss_per_iter = []\n",
    "loss_per_batch = []\n",
    "\n",
    "\n",
    "# Train the net\n",
    "losses = []\n",
    "auc_train = []\n",
    "auc_test = []\n",
    "metric_val = []\n",
    "unc_val = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T19:01:10.366383Z",
     "start_time": "2021-11-26T18:57:58.262529Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(44.917991668810934, 152.6603901697268)\n",
      "5\n",
      "(44.92483150454373, 152.6524620848051)\n",
      "10\n",
      "(44.90500162184589, 152.63166762212677)\n",
      "15\n",
      "(44.992089541383066, 152.6528160025033)\n",
      "20\n",
      "(44.915178884036514, 152.60940857305727)\n",
      "25\n",
      "(44.90979941463607, 152.59701335301145)\n",
      "30\n",
      "(44.848592242900914, 152.56300478738092)\n",
      "35\n",
      "(44.88603464731981, 152.5687273746578)\n",
      "40\n",
      "(44.86706725932372, 152.54818912835512)\n",
      "45\n",
      "(44.7988124236224, 152.5058869306503)\n",
      "50\n",
      "(44.84391457182361, 152.51040995495285)\n",
      "55\n",
      "(44.863748528438784, 152.51401935215316)\n",
      "60\n",
      "(44.837740698308956, 152.49499142282244)\n",
      "65\n",
      "(44.827804711566316, 152.47626091796826)\n",
      "70\n",
      "(44.872486835314945, 152.48333247386958)\n",
      "75\n",
      "(44.74739716627189, 152.4188359902757)\n",
      "80\n",
      "(44.82409348427972, 152.44208587152025)\n",
      "85\n",
      "(44.82232914953445, 152.42188784986388)\n",
      "90\n",
      "(44.84146415484183, 152.4195402613783)\n",
      "95\n",
      "(44.747119247159766, 152.37521250753494)\n",
      "Elapsed time:  1016.5806739330292\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# hyperparameteres\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(trainloader):\n",
    "        X = inputs.to(device)\n",
    "        y = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forwarde\n",
    "        outputs = nnet(X.float())\n",
    "\n",
    "        # Compute diff\n",
    "\n",
    "        loss = interval_score_loss(outputs, y.float())\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save loss to plot\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print(epoch)\n",
    "        auc_train.append(loss.cpu().detach().numpy())\n",
    "        pred = nnet(X_test.float())\n",
    "        auc_test.append(interval_score_loss(pred, y_test.float()).detach().numpy())\n",
    "\n",
    "        preds = torch.mean(nnet(X_test.float()), axis=1).cpu().detach().numpy()\n",
    "        lower = nnet(X_test.float())[:, 0].cpu().detach().numpy()\n",
    "        upper = nnet(X_test.float())[:, 1].cpu().detach().numpy()\n",
    "\n",
    "        val_preds_df = (\n",
    "            df_feats.query(\"validation == 1\")\n",
    "            .loc[:, [\"month\", \"region\", \"brand\"]]\n",
    "            .assign(sales=preds)\n",
    "            .assign(lower=lower)\n",
    "            .assign(upper=upper)\n",
    "        )\n",
    "\n",
    "        ground_truth_val = df_feats.query(\"validation == 1\").loc[\n",
    "            :, [\"month\", \"region\", \"brand\", \"sales\"]\n",
    "        ]\n",
    "        res = ComputeMetrics(val_preds_df.fillna(0), sales_train, ground_truth_val)\n",
    "        print(res)\n",
    "\n",
    "\n",
    "        metric_val.append(res[0])\n",
    "        unc_val.append(res[1])\n",
    "\n",
    "        # Figure\n",
    "        plt.figure()\n",
    "        plt.plot(auc_train, label=\"train\")\n",
    "        plt.plot(auc_test, label=\"test\")\n",
    "        plt.plot(metric_val, label=\"Metric\")\n",
    "        plt.plot(unc_val, label=\"Uncertainty\")\n",
    "        plt.legend()\n",
    "        plt.ylim([0, 200])\n",
    "        plt.savefig(\"output/auc_NN.png\")\n",
    "        plt.savefig(\"output/auc_NN.svg\", format=\"svg\")\n",
    "        plt.close()\n",
    "\n",
    "        #\n",
    "        path = \"output/weights\" + str(epoch) + \".pt\"\n",
    "        torch.save(nnet.state_dict(), path)\n",
    "\n",
    "print(\"Elapsed time: \", np.abs(tic - time.time()))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:55:57.096633Z",
     "start_time": "2021-11-26T18:55:57.047581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3164, 12])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkX_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.572059Z",
     "start_time": "2021-11-26T18:44:09.240Z"
    }
   },
   "outputs": [],
   "source": [
    "def curation_post(data):\n",
    "    aux = data.copy()\n",
    "\n",
    "    # Save arrays\n",
    "    aux_low = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_low\n",
    "    aux_high = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].pred_95_high\n",
    "    aux_index = aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].index\n",
    "\n",
    "    # Modify\n",
    "    aux.loc[aux_index, \"pred_95_low\"] = aux_high\n",
    "    aux.loc[aux_index, \"pred_95_high\"] = aux_low\n",
    "    \n",
    "    \n",
    "    if aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape[0]>0:\n",
    "        print('If errors they should appear: ')\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "        print(aux[aux[\"pred_95_high\"] < aux[\"pred_95_low\"]].shape)\n",
    "\n",
    "    preds_aux = np.mean([aux.pred_95_low, aux.pred_95_high], axis=0)\n",
    "    \n",
    "    aux['prediction'] = preds_aux\n",
    "    return aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.574003Z",
     "start_time": "2021-11-26T18:44:09.242Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_withNN(NN, data):\n",
    "    # Make predictions\n",
    "    preds = torch.mean(NN(X_test.float()), axis=1).detach().numpy()\n",
    "    lower = NN(X_test.float())[:, 0].detach().numpy()\n",
    "    upper = NN(X_test.float())[:, 1].detach().numpy()\n",
    "\n",
    "    print(my_metric(preds, lower, upper))\n",
    "\n",
    "    # Modify offset\n",
    "    \n",
    "    #preds = (preds + 1) * val_offset\n",
    "    #lower = (lower + 1) * val_offset\n",
    "    #upper = (upper + 1) * val_offset\n",
    "\n",
    "    aux_data = data.copy()\n",
    "\n",
    "    aux_data[\"prediction\"] = preds\n",
    "    aux_data[\"pred_95_low\"] = lower\n",
    "    aux_data[\"pred_95_high\"] = upper\n",
    "\n",
    "    aux_data = curation_post(aux_data)\n",
    "    return aux_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.576102Z",
     "start_time": "2021-11-26T18:44:09.244Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet60 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet60.load_state_dict(torch.load(\"output/weights60.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.577808Z",
     "start_time": "2021-11-26T18:44:09.248Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet80 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet80.load_state_dict(torch.load(\"output/weights80.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.579591Z",
     "start_time": "2021-11-26T18:44:09.250Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet95 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet95.load_state_dict(torch.load(\"output/weights95.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.581059Z",
     "start_time": "2021-11-26T18:44:09.252Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet100 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet100.load_state_dict(torch.load(\"output/weights100.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.583311Z",
     "start_time": "2021-11-26T18:44:09.257Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet120 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet120.load_state_dict(torch.load(\"output/weights140.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.584782Z",
     "start_time": "2021-11-26T18:44:09.260Z"
    }
   },
   "outputs": [],
   "source": [
    "nnet125 = ResNet(trainset.__shape__()).to(device)\n",
    "nnet125.load_state_dict(torch.load(\"output/weights125.pt\",map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T14:46:00.158649Z",
     "start_time": "2020-11-28T14:46:00.142701Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T14:47:03.603639Z",
     "start_time": "2020-11-28T14:47:03.590667Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.586346Z",
     "start_time": "2021-11-26T18:44:09.267Z"
    }
   },
   "outputs": [],
   "source": [
    "n_60 = predict_withNN(nnet60,val_x_orig[['country','brand','month_num']])\n",
    "n_80 = predict_withNN(nnet80,val_x_orig[['country','brand','month_num']])\n",
    "n_95 = predict_withNN(nnet95,val_x_orig[['country','brand','month_num']])\n",
    "n_100 = predict_withNN(nnet100,val_x_orig[['country','brand','month_num']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.588027Z",
     "start_time": "2021-11-26T18:44:09.269Z"
    }
   },
   "outputs": [],
   "source": [
    "n_val_final = n_100.copy()\n",
    "\n",
    "n_val_final.prediction = np.mean([n_60.prediction,\n",
    "                                n_80.prediction,\n",
    "                                n_95.prediction,\n",
    "                                n_100.prediction],axis=0)\n",
    "\n",
    "n_val_final.pred_95_low = np.mean([n_60.pred_95_low,\n",
    "                                n_80.pred_95_low,\n",
    "                                n_95.pred_95_low,\n",
    "                                n_100.pred_95_low],axis=0)\n",
    "\n",
    "n_val_final.pred_95_high = np.mean([n_60.pred_95_high,\n",
    "                                n_80.pred_95_high,\n",
    "                                n_95.pred_95_high,\n",
    "                                n_100.pred_95_high],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.589885Z",
     "start_time": "2021-11-26T18:44:09.274Z"
    }
   },
   "outputs": [],
   "source": [
    "my_metric(n_val_final.prediction,\n",
    "         n_val_final.pred_95_low,\n",
    "         n_val_final.pred_95_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.592320Z",
     "start_time": "2021-11-26T18:44:09.281Z"
    }
   },
   "outputs": [],
   "source": [
    "n_val_final.to_csv('output/valid_ensemble.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.594301Z",
     "start_time": "2021-11-26T18:44:09.282Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = torch.mean(nnet(X_test.float()),axis=1).detach().numpy()\n",
    "lower = nnet125(X_test.float())[:, 0].detach().numpy()\n",
    "upper = nnet125(X_test.float())[:, 1].detach().numpy()\n",
    "\n",
    "preds = (preds+1)*val_offset\n",
    "lower = (lower+1)*val_offset\n",
    "upper = (upper+1)*val_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.596095Z",
     "start_time": "2021-11-26T18:44:09.284Z"
    }
   },
   "outputs": [],
   "source": [
    "val_preds = val_x\n",
    "\n",
    "val_preds[\"prediction\"] = preds\n",
    "val_preds[\"pred_95_low\"] = lower\n",
    "val_preds[\"pred_95_high\"] = upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.597932Z",
     "start_time": "2021-11-26T18:44:09.288Z"
    }
   },
   "outputs": [],
   "source": [
    "def submission_predict(NN):\n",
    "\n",
    "    submission_df = pd.read_csv(\"../data/submission_template.csv\")\n",
    "\n",
    "    preds = torch.mean(NN(torch.tensor(test_x.values).float()), axis=1).detach().numpy()\n",
    "    lower = NN(torch.tensor(test_x.values).float())[:, 0].detach().numpy()\n",
    "    upper = NN(torch.tensor(test_x.values).float())[:, 1].detach().numpy()\n",
    "\n",
    "    preds = (preds + 1) * test_offset\n",
    "    lower = (lower + 1) * test_offset\n",
    "    upper = (upper + 1) * test_offset\n",
    "\n",
    "    submission_df[\"pred_95_low\"] = np.maximum(lower, 0)\n",
    "    submission_df[\"pred_95_high\"] = np.maximum(upper, 0)\n",
    "    submission_df[\"prediction\"] = np.maximum(preds, 0)\n",
    "    submission_df = curation_post(submission_df)\n",
    "\n",
    "\n",
    "\n",
    "    e = submission_df[\n",
    "        submission_df[\"pred_95_high\"] < submission_df[\"pred_95_low\"]\n",
    "    ].shape[0]\n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "\n",
    "    e = submission_df[\n",
    "        submission_df[\"pred_95_low\"] > submission_df[\"pred_95_high\"]\n",
    "    ].shape[0]\n",
    "    print(submission_df[\n",
    "        submission_df[\"pred_95_low\"] > submission_df[\"pred_95_high\"]\n",
    "    ])\n",
    "    \n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "\n",
    "    e = submission_df[\n",
    "        submission_df[\"prediction\"] > submission_df[\"pred_95_high\"]\n",
    "    ].shape[0]\n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "\n",
    "    e = submission_df[submission_df[\"prediction\"] < submission_df[\"pred_95_low\"]].shape[\n",
    "        0\n",
    "    ]\n",
    "    if e > 0:\n",
    "        print(\"WARNING:ERORR, please debug\")\n",
    "        \n",
    "        \n",
    "    submission_df = postprocess_submission(submission_df)\n",
    "\n",
    "    submission_df[\"pred_95_low\"] = np.maximum(submission_df.pred_95_low, 0)\n",
    "    submission_df[\"pred_95_high\"] = np.maximum(submission_df.pred_95_high, 0)\n",
    "    submission_df[\"prediction\"] = np.maximum(submission_df.prediction, 0)\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.599847Z",
     "start_time": "2021-11-26T18:44:09.294Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred60 = submission_predict(nnet60)\n",
    "pred80 = submission_predict(nnet80)\n",
    "pred95 = submission_predict(nnet95)\n",
    "pred100 = submission_predict(nnet100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.601846Z",
     "start_time": "2021-11-26T18:44:09.298Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_final = pred60.copy()\n",
    "\n",
    "pred_final.prediction = np.mean(\n",
    "    [pred60.prediction, pred80.prediction, pred95.prediction, pred100.prediction],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "pred_final.pred_95_low = np.mean(\n",
    "    [pred60.pred_95_low, pred80.pred_95_low, pred95.pred_95_low, pred100.pred_95_low],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "pred_final.pred_95_high = np.mean(\n",
    "    [\n",
    "        pred60.pred_95_high,\n",
    "        pred80.pred_95_high,\n",
    "        pred95.pred_95_high,\n",
    "        pred100.pred_95_high,\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.603398Z",
     "start_time": "2021-11-26T18:44:09.303Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.604882Z",
     "start_time": "2021-11-26T18:44:09.309Z"
    }
   },
   "outputs": [],
   "source": [
    "pred60.to_csv(\"../submissions/pred60_noPost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.607073Z",
     "start_time": "2021-11-26T18:44:09.314Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_final.to_csv(\"../submissions/pred_final_few.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.608852Z",
     "start_time": "2021-11-26T18:44:09.315Z"
    }
   },
   "outputs": [],
   "source": [
    "train_x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.611255Z",
     "start_time": "2021-11-26T18:44:09.319Z"
    }
   },
   "outputs": [],
   "source": [
    "a = pd.read_csv('../data/gx_merged_lags_monthsD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.612960Z",
     "start_time": "2021-11-26T18:44:09.325Z"
    }
   },
   "outputs": [],
   "source": [
    "b = pd.read_csv('../data/gx_merged_lags_monthsM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.614561Z",
     "start_time": "2021-11-26T18:44:09.329Z"
    }
   },
   "outputs": [],
   "source": [
    "a['last_before_3_after_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.616862Z",
     "start_time": "2021-11-26T18:44:09.331Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(b['last_before_3_after_0_vMarc'] != b['last_before_3_after_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.618644Z",
     "start_time": "2021-11-26T18:44:09.332Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(a==b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T18:44:32.620610Z",
     "start_time": "2021-11-26T18:44:09.336Z"
    }
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "novartis",
   "language": "python",
   "name": "novartis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
